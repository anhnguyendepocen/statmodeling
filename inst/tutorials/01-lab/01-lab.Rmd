---
title: "Lab: R Math & Programming Refresher"
output: 
  learnr::tutorial:
    css: css/learnr-theme.css
runtime: shiny_prerendered
---

```{r 01-lab-setup, include=FALSE}
if ("learnr" %in% (.packages()))
  detach(package:learnr, unload = TRUE)
library(learnr)
knitr::opts_chunk$set(echo = FALSE)

## Save package names as a vector of strings
pkgs <-  c("foreign",
           "devtools")

## Install uninstalled packages
lapply(pkgs[!(pkgs %in% installed.packages())], 
       install.packages,
       repos='http://cran.us.r-project.org')

## Load all packages to library and adjust options
lapply(pkgs, library, character.only = TRUE)
```


## Doing OLS by hand

We have all used the ordinary least squares estimator for linear regression.
But have yo ever implemented it by hand? This will be your task for this exercise! At the same time, it will be a segue into Session 2 of our workshop, which focuses on generalized linear models.

Note that we will not *derive* the OLS estimator by hand. You will get the derivation and translate it into R.

The exercise consists of two parts:

### Part 1

1. Data preparation
2. Estimation

### Part 2

1. Processing ("Prediction")
2. Visualization


### Notes

- Note that this exercise may be challenging at times, but I have supplied
numerous hints for you. 
- If you get stuck, feel free to step outside the breakout room and ask me for additional hints in the Zoom meeting.
- You can always skip a task. Even if serial tasks rely on object you are prompted to generate, the correct objects will be preloaded in all subsequent steps.

## The linear model

The linear model is a special case of generalized linear models (more on this in the next session).

The OLS estimator for multivariate regression analyses requires
calculations in matrix form. 

### Model specification:

Likelihood:
$$\mathbf{y}_i \sim \text{N}(\mu_i, \sigma^2)$$

Systematic component:
$$\mu_i = \mathbf{x}_i^{\prime} \beta$$

### Four way to denote the linear model formula

1. Scalar form: $$y_i = \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i \text{ for all } i=1,...,N$$
2. Row-vector form: $$y_i = \mathbf{x_i^{\prime}} \mathbf{\beta} + \epsilon_i  \text{ for all } i=1,...,N$$
3. Column-vector form: $$\mathbf{y} = \beta_1 \mathbf{x_{1}} + \beta_2 \mathbf{x_{2}} + \beta_3 \mathbf{x_{3}} + \mathbf{\epsilon}$$
4. Matrix form: $$\mathbf{y = X \beta + \epsilon}$$

### What the matrix-form looks like in detail

$$\underbrace{\begin{bmatrix} y_1 \\ \vdots \\  y_{N} \end{bmatrix}}_{N \times 1}
		=
		\underbrace{\begin{bmatrix}
		1 			& x_{1, 2}	 & \cdots &  x_{1, K} \\ 
		\vdots 		& \vdots  	 & \ddots &  \vdots \\
		1 			& x_{N, 2}    & \cdots & x_{N, K}
		\end{bmatrix}}_{N \times K}
		\underbrace{\begin{bmatrix} \beta_1 \\ \beta_2 \\ \vdots \\ \beta_K \end{bmatrix}}_{K \times 1}
		+ 
	\underbrace{\begin{bmatrix} \epsilon_1 \\ \vdots \\ \epsilon_{N} \end{bmatrix}}_{N \times 1}$$

### The matrix form is 100% equivalent to the other three forms

$$\underbrace{\begin{bmatrix} y_1 \\ \vdots \\  y_{N} \end{bmatrix}}_{N \times 1}
	=
	\underbrace{\begin{bmatrix}
	\beta_1 \cdot 1	+ \beta_2 \cdot x_{1, 2} + \cdots + \beta_K \cdot x_{1, K} \\
	\vdots \\
	\beta_1 \cdot 1	+ \beta_2 \cdot x_{N, 2} + \cdots + \beta_K \cdot x_{N, K} \\
	\end{bmatrix}}_{N \times 1}
	+ 
	\underbrace{\begin{bmatrix} \epsilon_1 \\ \vdots \\ \epsilon_{N} \end{bmatrix}}_{N \times 1}$$
	
## Inputs and outputs

### Inputs (data)

1. A length-$N$ outcome vector $\mathbf{y}$
1. An $N \times K$ model matrix $\mathbf{X}$

### Primary outputs (estimands, parameters)

1. A length-$K$ coefficient vector $\beta$
1. A real, strictly positive variance parameter $\sigma^2$

### Secondary outputs (transformed parameters)

With estimates $\hat{\beta}$ and $\hat{\sigma}^2$, we can directly calculate the variance-covariance matrix of the coefficients, $\hat{\mathbf{\Sigma}}$. This variance-covariance matrix gives us the parameters that define the joint multivariate normal *sampling distribution* of our coefficients:
$$\hat{\beta} \sim \text{MVN}(\hat{\beta}, \hat{\mathbf{\Sigma}})$$
Some things to note:

- The diagonal of this matrix holds the *variances* of our coeffients.
- The square root of these variances gives us their *standard errors*.
- The off-diagonals give us the *covariances* of any pair of coefficients.
- The variance-covariance matrix is of dimensions $K \times K$.
- The variance-covariance matrix is *symmetrical*; thus $\Sigma_{a,b} = \Sigma{a, b}$ for any $a, b \in \{1,...,K\}$.


## Data preparation

The exercise chunk has pre-loaded the data frame `gles`, which three variables from the 2017 German Longitudinal Election Study. We want to model respondents' support for the AfD (`sup_afd`, measured on an 11-point scale ranging from -5 to 5) as a function of respondents' socio-economic preferences (`se_self`), immigration preferences (`la_self`), including a multiplicative interaction term for the two predictors (`se_self:la_self`). 

Both `se_self` and `la_self` are measured on 11-point scales. `se_self` ranges from values (0) "less taxes and deductions, even if
that means less social spending" to (10) "more social spending, even if that means
more taxes and deductions". `la_self`ranges from values (0) "facilitate immigration" to (10) "restrict immigration".

The model formula is thus given by
$$\mathtt{sup\_afd} = \beta_1 + \beta_2 \mathtt{se\_self} + \beta_3 \mathtt{la\_self} + \beta_4 \mathtt{se\_self:la\_self} + \epsilon$$
The task is simple: We need the vector $\mathbf{y}$ and the matrix $\mathbf{X}$

Hints:

- You can extract any column from a data frame using the `$` operator.
- Check out `model.matrix()`, which takes the right-hand of a model formula as input and returns a complete model matrix.

```{r data-prep-setup}
library(foreign)
gles <- read.dta("data/gles.dta")
```

```{r data-prep, exercise=TRUE, exercise.setup="data-prep-setup"}
y <- 
X <- 
```

## Estimation

### The coefficient vector

You have probably seen this before: The OLS estimator for $\beta$ is given by $$\hat{\beta} = \mathbf{(X^{\prime}X)^{-1}X^{\prime}y}$$

Let's translate this into R! Hints:

- `t()` transposes matrices/vectors
- `solve()` inverts matrices
- `%*%` multiplies matrices vectors

### The variance parameter

The estimator for the variance parameter $\sigma^2$ is
$$\hat{\sigma}^2 = \frac{\mathbf{e}^{\prime}\mathbf{e}}{N-K}$$

Here, $\mathbf{e}$ is a length-$N$ of residuals, where $\mathbf{e} = \mathbf{y} - \mathbf{X}\beta$, which is the same as $\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}}$.

For this task, first calculate $\mathbf{e}$. Then, calculate $\hat{\sigma}^2$. After this, you can calculate the variance-covariance matrix, which is given by
$$\hat{\mathbf{\Sigma}} = \hat{\sigma}^2 \mathbf{(X^{\prime}X)^{-1}} $$

Hints:

- Remember that X is of dimensions $N \times K$. Try `dim()`, `nrow()`, and `ncol`.


## Processing (prediction)

With $\hat\beta$ and $\hat{\mathbf{\Sigma}}$, we have knowledge of the full sampling distribution for $\hat\beta$. This allows us to also get confidence intervals for any post-estimation quantities of interest that are functions of parameters and data.

We are going to focus on *expected values*: $\mathbb{E}[y|\mathbf{\mathbf{x}^{\ast}}] = \mathbf{x}^{\ast\prime}\hat\beta$. The vector $\mathbf{x}^{\ast}$ can contain any (reasonable) scenario of covariate values. Here, we want to calculate expected values over the full grid of values of `se_self` and `la_self`.

How can we get it's standard error? In OLS models, the variance of an expected value is given by $$\mathbf{x}^{\ast} \hat{\mathbf{\Sigma}} \mathbf{x}^{\ast \prime}$$. It's square root gives the standard error.

### Step 1: Define value sequences for the predictors

Remember that both predictors are measured on 11-point scales ranging from 0 to 10. But that shouldn't stop us from calculating at finer gradations of the two scales. Define value sequences ranging from 0 to 10 in steps of 0.25 for both predictors (Hint: `seq()`) and store them in the vectors below.

### Step 2: Initialize a container

What do we want eventually? 

### Step 3: Use a nested loop to fill the container

Hints:
- Your outer loop should loop through the elements of `se_vals` (see `seq_along()`).
- Your inner loop should loop through the elements of `la_vals` (see `seq_along()`).
- Define (overwrite) the vector `x_star` at each iteration of the loop. Remember that our model matrix has four columns -- you will need a corresponding element 
    1. ones multiplying the intercept
    1. values of `se_self`
    1. values of `la_self`
    1. values of the product term `se_self:la_self`
    



## Visualization

Time for some shameless self-promotion: We are going to download my `regplane3D` and
visualize the expected values of AfD support as a joint function of socio-economic
and immigration preferences.







```{r 01-lab-two-plus-two, exercise=TRUE}

```
